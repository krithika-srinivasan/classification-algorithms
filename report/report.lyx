#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\date{}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
CSE 601 - Classification Algorithms Report
\end_layout

\begin_layout Author
Dipack P Panjabi (50291077), Krithika Srinivasan (-)
\end_layout

\begin_layout Section
Overview
\end_layout

\begin_layout Standard
This project is about implementing 4 classification algorithms, and evaluating
 their performance using 10 fold cross validation, measuring accuracy, precision
, recall, and F-1 measure.
 The 4 algorithms are k-Nearest Neighbour, Decision Tree, Naive Bayes, and
 Random Forest.
 The algorithms are tested on 2 provided data sets.
\end_layout

\begin_layout Section
Implementation
\end_layout

\begin_layout Subsection
Cross Validation
\end_layout

\begin_layout Standard
We perform 10 fold cross validation on each of the 4 algorithms to measure
 their performance.
\end_layout

\begin_layout Standard
The algorithm works as follows,
\end_layout

\begin_layout Enumerate
Partition the data set into training and validation subsets.
 We do this by using the concept of a sliding window to partition the total
 dataset into each type.
 In this case, as we use 10 fold cross validation, the validation dataset
 is 10% of the total dataset, with the rest used for training.
\end_layout

\begin_deeper
\begin_layout Enumerate
If we for example we have a total of 500 samples in the dataset, the first
 50 samples are the validation set for the first fold, with the remaining
 used for training.
 The second fold uses samples indexed from 50 to 100 as the validation set,
 with samples indexed 0-49, and 101-500 as the training, following the sliding
 window principle.
\end_layout

\begin_layout Enumerate
We use this technique to divide the data into 10 sets, with each set containing
 a validation and training dataset.
\end_layout

\end_deeper
\begin_layout Enumerate
Once we have the training and validation datasets, we train the learner
 (e.g.
 Decision Tree) on the training dataset, and evaluate its performance by
 asking it to predict the class labels for both training and validation
 datasets.
\end_layout

\begin_layout Enumerate
Once we have all predictions, we calculate the accuracy, precision, recall,
 and F-1 scores for both datasets.
\end_layout

\begin_layout Enumerate
Repeat steps 1-3 for each fold.
\end_layout

\begin_layout Subsection
Decision Tree
\end_layout

\begin_layout Standard
The decision tree algorithm works as follows,
\end_layout

\begin_layout Enumerate
Based on the data we have currently, we select the best feature to split
 the data into two parts.
 The best feature is selected by splitting the data along each feature,
 for each feature value, and selecting the feature, and feature value, that
 gives us the highest Gini index.
 We also do not use each feature value more than once.
\end_layout

\begin_deeper
\begin_layout Enumerate
A high Gini index means that the data is better distributed between both
 branches of the binary decision tree node.
 This is important in building a balanced decision tree with enough samples
 on either branch.
\end_layout

\begin_layout Enumerate
Gini index for a tree node can be calculated as follows
\begin_inset Formula 
\[
GINI(x)=1-\sum_{j}[\ p(j|x)\ ]^{2}
\]

\end_inset

where 
\begin_inset Formula $x$
\end_inset

 is the tree node, 
\begin_inset Formula $p(j|x)$
\end_inset

 is the relative frequency of class 
\begin_inset Formula $j$
\end_inset

 at node 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Once we have the best feature split, we split the data set into two branches,
 and then repeat step 1 on each of the branches, to give us further splits.
 We repeat this process until we reach the maximum specified tree depth,
 or we simply do not have the minimum required number of samples to justify
 splitting the dataset any further.
\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand lstinputlisting
filename "example_decision_tree.txt"
lstparams "breaklines=true,captionpos=b,frame=tb,caption={Example Decision Tree}"

\end_inset


\end_layout

\begin_layout Subsubsection
Observations
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Data set
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accuracy (Train/Validation)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Precision (T/V)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recall (T/V)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
F-1 score (T/V)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
dataset1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.992/0.916
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.993/0.891
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.985/0.886
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.989/0.886
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
dataset2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.693/0.673
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.650/0.368
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.480/0.426
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.449/0.392
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Decision Tree performance scores
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The max depth was set to 6 levels, and min number of samples required to
 split set to 2 samples, for both data sets.
 We found that these settings give us the best balance between training
 and validation dataset performance.
\end_layout

\begin_layout Standard
Applying the decision tree algorithm to both sets shows us that the decision
 tree approach works really well, especially for the first dataset.
 The second dataset gives us a lower accuracy using decision trees, which
 could be the presence of a non-numerical attribute, or more cohesive set
 of data points, making them difficult to separate.
\end_layout

\begin_layout Standard
Overall, the simple nature of decision trees, along with their relatively
 easy implementation and prediction approach, makes this a good algorithm
 to use for classification, especially if the data set is easier to split.
\end_layout

\begin_layout Standard
The downside of decision trees is their brute force approach to computing
 the best feature split at each level in the tree, which makes them scale
 poorly for larger tree and dataset sizes, and the fact that it can easily
 overfit to the training data.
\end_layout

\begin_layout Subsection
Random Forest
\end_layout

\begin_layout Standard
The random forest approach utilises multiple decision trees, each fitted
 on a different subset of the input dataset, to predict the final class
 labels.
 Each split in a decision tree is also selected from a random subset of
 the original feature set, different from a regular decision tree which
 selects the best split from the entire feature set.
\end_layout

\begin_layout Standard
We select the subset of the dataset to train each of our decision trees
 on, by sampling the original training set with replacement.
 This technique is called bootstrap aggregation.
\end_layout

\begin_layout Standard
Since this algorithm utilises multiple decision trees, we get a predicted
 class label from each tree, with the final predicted class label chosen
 using a simple majority vote.
\end_layout

\begin_layout Subsubsection
Observations
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Data set
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accuracy (Train/Validation)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Precision (T/V)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recall (T/V)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
F-1 score (T/V)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
dataset1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.984/0.955
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.997/0.970
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.961/0.910
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.979/0.938
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
dataset2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.752/0.693
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.801/0.641
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.399/0.310
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.520/0.386
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Random Forest performance scores
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
50 trees were planted in the random forest, with the max depth set to 5
 levels, min number of samples required to split set to 2 samples, both
 the feature and sample sampling ratios set to 40% for dataset 1.
 We plant a 100 trees, for dataset 2, with all the other settings identical
 to dataset 1.
 We found that these settings give us the best balance between training
 and validation dataset performance.
\end_layout

\begin_layout Standard
For the first dataset, we see that all 4 metrics are within range of each
 other, pointing to the fact that the particular dataset is easily divisible.
\end_layout

\begin_layout Standard
For the second dataset, we see that the most values, except for recall,
 are better than the decision tree approach, but not by much, whereas the
 first dataset gives much better results compared to its decision tree counterpa
rt.
 This once again points to the fact that the second dataset is more cohesive,
 making it harder to find a good decision boundary.
 The lower recall value could be explained by the fact that we have many
 trees, and the average recall value could be skewed by false negatives
 classified by many trees.
\end_layout

\begin_layout Standard
Where the random forest approach truly shines is in consistency when testing
 against datasets other than the one it was trained on.
 This is because the multiple independent trees, and the subset approach
 to fit both the dataset and feature set, help the forest learn a more intricate
 decision boundary, and reduces the chances of the entire forest overfitting
 to the training dataset.
 This, contrasted with the fact that a lone decision tree can only learn
 a linear decision boundary, gives random forest the edge.
\end_layout

\begin_layout Standard
The downside of random forests is the correspondingly longer training and
 prediction times compared to a lone decision tree, but in the grand scheme
 of things, it still takes lesser time to do its job compared to more complicate
d algorithms, with results that are just as good (if not better) than those
 same algorithms.
\end_layout

\end_body
\end_document
