%% LyX 2.3.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{float}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\date{}

\makeatother

\usepackage{babel}
\usepackage{listings}
\begin{document}
\title{CSE 601 - Classification Algorithms Report}
\author{Dipack P Panjabi (50291077), Krithika Srinivasan (-)}
\maketitle

\section{Overview}

This project is about implementing 4 classification algorithms, and
evaluating their performance using 10 fold cross validation, measuring
accuracy, precision, recall, and F-1 measure. The 4 algorithms are
k-Nearest Neighbour, Decision Tree, Naive Bayes, and Random Forest.
The algorithms are tested on 2 provided data sets.

\section{Implementation}

\subsection{Cross Validation}

We perform 10 fold cross validation on each of the 4 algorithms to
measure their performance.

The algorithm works as follows,
\begin{enumerate}
\item Partition the data set into training and validation subsets. We do
this by using the concept of a sliding window to partition the total
dataset into each type. In this case, as we use 10 fold cross validation,
the validation dataset is 10\% of the total dataset, with the rest
used for training.
\begin{enumerate}
\item If we for example we have a total of 500 samples in the dataset, the
first 50 samples are the validation set for the first fold, with the
remaining used for training. The second fold uses samples indexed
from 50 to 100 as the validation set, with samples indexed 0-49, and
101-500 as the training, following the sliding window principle.
\item We use this technique to divide the data into 10 sets, with each set
containing a validation and training dataset.
\end{enumerate}
\item Once we have the training and validation datasets, we train the learner
(e.g. Decision Tree) on the training dataset, and evaluate its performance
by asking it to predict the class labels for both training and validation
datasets.
\item Once we have all predictions, we calculate the accuracy, precision,
recall, and F-1 scores for both datasets.
\item Repeat steps 1-3 for each fold.
\end{enumerate}

\subsection{Decision Tree}

The decision tree algorithm works as follows,
\begin{enumerate}
\item Based on the data we have currently, we select the best feature to
split the data into two parts. The best feature is selected by splitting
the data along each feature, for each feature value, and selecting
the feature, and feature value, that gives us the highest Gini index.
\begin{enumerate}
\item A high Gini index means that the data is better distributed between
both branches of the binary decision tree node. This is important
in building a balanced decision tree with enough samples on either
branch.
\item Gini index for a tree node can be calculated as follows
\[
GINI(x)=1-\sum_{j}[\ p(j|x)\ ]^{2}
\]
where $x$ is the tree node, $p(j|x)$ is the relative frequency of
class $j$ at node $x$.
\end{enumerate}
\item Once we have the best feature split, we split the data set into two
branches, and then repeat step 1 on each of the branches, to give
us further splits. We repeat this process until we reach the maximum
specified tree depth, or we simply do not have the minimum required
number of samples to justify splitting the dataset any further.
\end{enumerate}
\lstinputlisting[breaklines=true,captionpos=b,frame=tb,caption={Example Decision Tree}]{example_decision_tree.txt}

\subsubsection{Observations}

\begin{table}[H]
\begin{tabular}{|c|c|c|c|c|}
\hline 
Data set & Accuracy (Train/Validation) & Precision (T/V) & Recall (T/V) & F-1 score (T/V)\tabularnewline
\hline 
\hline 
dataset1 & 0.992/0.916 & 0.993/0.891 & 0.985/0.886 & 0.989/0.886\tabularnewline
\hline 
dataset2 & 0.693/0.673 & 0.650/0.368 & 0.480/0.426 & 0.449/0.392\tabularnewline
\hline 
\end{tabular}

\caption{Decision Tree performance scores}

\end{table}

The max depth was set to 6 levels, and min number of samples required
to split set to 2 samples, for both data sets. We found that these
settings give us the best balance between training and validation
dataset performance.

Applying the decision tree algorithm to both sets shows us that the
decision tree approach works really well, especially for the first
dataset. The second dataset gives us a lower accuracy using decision
trees, which could be the presence of a non-numerical attribute, or
more cohesive set of data points, making them difficult to separate.

Overall, the simple nature of decision trees, along with their relatively
easy implementation and prediction approach, makes this a good algorithm
to use for classification, especially if the data set is easier to
split.

The downside of decision trees is their brute force approach to computing
the best feature split at each level in the tree, which makes them
scale poorly for larger tree and dataset sizes, and the fact that
it can easily overfit to the training data.

\subsection{Random Forest}

The random forest approach utilises multiple decision trees, each
fitted on a different subset of the input dataset, to predict the
final class labels. Each split in a decision tree is also selected
from a random subset of the original feature set, different from a
regular decision tree which selects the best split from the entire
feature set.

Since this algorithm utilises multiple decision trees, we get a predicted
class label from each tree, with the final predicted class label chosen
using a simple majority vote.

\subsubsection{Observations}

\begin{table}[H]
\begin{tabular}{|c|c|c|c|c|}
\hline 
Data set & Accuracy (Train/Validation) & Precision (T/V) & Recall (T/V) & F-1 score (T/V)\tabularnewline
\hline 
\hline 
dataset1 & 0.984/0.955 & 0.997/0.970 & 0.961/0.910 & 0.979/0.938\tabularnewline
\hline 
dataset2 & 0.752/0.693 & 0.832/0.652 & 0.336/0.268 & 0.499/0.368\tabularnewline
\hline 
\end{tabular}

\caption{Random Forest performance scores}
\end{table}

50 trees were planted in the random forest, with the max depth set
to 5 levels, min number of samples required to split set to 2 samples,
both the feature and sample sampling ratios set to 40\% for dataset
1. We plant a 100 trees, for dataset 2, with all the other settings
identical to dataset 1. We found that these settings give us the best
balance between training and validation dataset performance.

For the first dataset, we see that all 4 metrics are within range
of each other, pointing to the fact that the particular dataset is
easily divisible.

For the second dataset, we see that the most values, except for recall,
are better than the decision tree approach, but not by much, whereas
the first dataset gives much better results compared to its decision
tree counterpart. This once again points to the fact that the second
dataset is more cohesive, making it harder to find a good decision
boundary. The lower recall value could be explained by the fact that
we have many trees, and the average recall value could be skewed by
false negatives classified by many trees.

Where the random forest approach truly shines is in consistency when
testing against datasets other than the one it was trained on. This
is because the multiple independent trees, and the subset approach
to fit both the dataset and feature set, help the forest learn a more
intricate decision boundary, and reduces the chances of the entire
forest overfitting to the training dataset. This, contrasted with
the fact that a lone decision tree can only learn a linear decision
boundary, gives random forest the edge.

The downside of random forests is the correspondingly longer training
and prediction times compared to a lone decision tree, but in the
grand scheme of things, it still takes lesser time to do its job compared
to more complicated algorithms, with results that are just as good
(if not better) than those same algorithms.
\end{document}
