%% LyX 2.3.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{float}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\date{}

\makeatother

\usepackage{babel}
\usepackage{listings}
\begin{document}
\title{CSE 601 - Classification Algorithms Report}
\author{Dipack P Panjabi (50291077), Krithika Srinivasan (50290373)}
\maketitle

\section{Overview}

This project is about implementing 4 classification algorithms, and
evaluating their performance using 10 fold cross validation, measuring
accuracy, precision, recall, and F-1 measure. The 4 algorithms are
k-Nearest Neighbour, Decision Tree, Naive Bayes, and Random Forest.
The algorithms are tested on 2 provided data sets.

\section{Implementation}

\subsection{Cross Validation}

We perform 10 fold cross validation on each of the 4 algorithms to
measure their performance.

The algorithm works as follows,
\begin{enumerate}
\item Partition the data set into training and validation subsets. We do
this by using the concept of a sliding window to partition the total
dataset into each fold. In this case, as we use 10 fold cross validation,
the validation dataset is 10\% of the total dataset, with the rest
used for training.
\begin{enumerate}
\item If, for example, we have a total of 500 samples in the dataset, the
first 50 samples are the validation set for the first fold, with the
remaining used for training. The second fold uses samples indexed
from 50 to 100 as the validation set, with samples indexed 0-49, and
101-500 as the training set, following the sliding window principle.
\item We use this technique to divide the data into 10 sets, with each set
containing a validation and training dataset.
\end{enumerate}
\item Once we have the training and validation datasets, we train the learner
(e.g. Decision Tree) on the training dataset, and evaluate its performance
by asking it to predict the class labels for the validation
dataset.
\item Once we have all predictions, we calculate the accuracy, precision,
recall, and F-1 scores for both datasets.
\item Repeat steps 1-3 for each fold.
\end{enumerate}

\subsection{Decision Tree}

The decision tree algorithm works as follows,
\begin{enumerate}
\item Based on the data we have currently, we select the best feature to
split the data into two parts. The best feature is selected by splitting
the data along each feature, for each feature value, and selecting
the feature, and feature value, that gives us the lowest Gini index.
We also do not use each feature value more than once.
\begin{enumerate}
\item A high Gini index means that the data is better distributed between
both branches of the binary decision tree node. This is important
in building a balanced decision tree with enough samples on either
branch. For example, if the Gini index is 0, that means all elements
belong to the same class, whereas a value of 1, means that elements
are randomly distributed across classes. We prefer the split that
has a lower Gini index.
\item Gini index for a tree node can be calculated as follows
\[
GINI(x)=1-\sum_{j}[\ p(j|x)\ ]^{2}
\]
where $x$ is the tree node, $p(j|x)$ is the relative frequency of
class $j$ at node $x$.
\end{enumerate}
\item Once we have the best feature split, we split the data set into two
branches, and then repeat step 1 on each of the branches, to give
us further splits. We repeat this process until we reach the maximum
specified tree depth, or we simply do not have the minimum required
number of samples to justify splitting the dataset any further.
\end{enumerate}
\lstinputlisting[breaklines=true,captionpos=b,frame=tb,caption={Example Decision Tree}]{example_decision_tree.txt}

\subsubsection{Feature Handling}

This decision tree handles continuous features such as floats in the
manner it receives them, i.e., it decides a split based on the actual
values provided to it.

Binary Categorical features, like the strings given in the two datasets,
are treated as binary variables, with each sample having either one
value or the other, allowing for binary splits.

The best feature chosen corresponds to the one with the lowest Gini
index at a given split point.

\subsubsection{Observations}

\begin{table}[H]
\begin{tabular}{|c|c|c|c|c|}
\hline 
Data set & Accuracy & Precision & Recall & F-1 score\tabularnewline
\hline 
\hline 
dataset1 & 0.930 & 0.903 & 0.915 & 0.908\tabularnewline
\hline 
dataset2 & 0.673 & 0.368 & 0.426 & 0.392\tabularnewline
\hline 
\end{tabular}

\caption{Decision Tree performance scores}

\end{table}

The max depth was set to 6 levels, and min number of samples required
to split set to 2 samples, for both data sets. We found that these
settings give us the best balance between training and validation
dataset performance.

Applying the decision tree algorithm to both sets shows us that the
decision tree approach works really well, especially for the first
dataset. The second dataset gives us a lower accuracy using decision
trees, which could be the presence of a non-numerical attribute, or
more cohesive set of data points, making them difficult to separate.

Overall, the simple nature of decision trees, along with their relatively
easy implementation and prediction approach, makes this a good algorithm
to use for classification, especially if the data set is easier to
split.

The downside of decision trees is their brute force approach to computing
the best feature split at each level in the tree, which makes them
scale poorly for larger tree and dataset sizes, and the fact that
it can easily overfit to the training data.

\subsection{Random Forest}

The random forest approach utilises multiple decision trees, each
fitted on a different subset of the input dataset, to predict the
final class labels. Each split in a decision tree is also selected
from a random subset of the original feature set, different from a
regular decision tree which selects the best split from the entire
feature set.

We select the subset of the dataset to train each of our decision
trees on, by sampling the original training set with replacement.
This technique is called bootstrap aggregation.

Since this algorithm utilises multiple decision trees, we get a predicted
class label from each tree, with the final predicted class label chosen
using a simple majority vote.

\subsubsection{Feature Handling}

Since Random Forests simply contain multiple Decision Trees, the features
are handled in exactly the same way as Decision trees.

\subsubsection{Observations}

\begin{table}[H]
\begin{tabular}{|c|c|c|c|c|}
\hline 
Data set & Accuracy & Precision & Recall & F-1 score\tabularnewline
\hline 
\hline 
dataset1 & 0.955 & 0.970 & 0.910 & 0.938\tabularnewline
\hline 
dataset2 & 0.702 & 0.747 & 0.279 & 0.372\tabularnewline
\hline 
\end{tabular}

\caption{Random Forest performance scores}
\end{table}

50 trees were planted in the random forest, with the max depth set
to 5 levels, min number of samples required to split set to 2 samples,
both the feature and sample sampling ratios set to 40\% for dataset
1. We plant 100 trees, for dataset 2, with all the other settings
identical to dataset 1. We found that these settings give us the best
balance between training and validation dataset performance.

For the first dataset, we see that all 4 metrics are within range
of each other, compared to the decision tree approach, pointing to
the fact that the particular dataset is easily divisible.

For the second dataset, we see that the most values, except for recall,
are better than the decision tree approach, but not by much, whereas
the first dataset gives much better results compared to its decision
tree counterpart. This once again points to the fact that the second
dataset is more cohesive, making it harder to find a good decision
boundary. The lower recall value could be explained by the fact that
we have many trees, and the average recall value could be skewed by
false negatives classified by many trees.

Where the random forest approach truly shines is in consistency when
testing against datasets other than the one it was trained on. This
is because the multiple independent trees, and the subset approach
to fit both the dataset and feature set, help the forest learn a more
intricate decision boundary, and reduces the chances of the entire
forest overfitting to the training dataset. This, contrasted with
the fact that a lone decision tree can only learn a linear decision
boundary, gives random forest the edge.

The downside of random forests is the correspondingly longer training
and prediction times compared to a lone decision tree, but in the
grand scheme of things, it still takes lesser time to do its job compared
to more complicated algorithms, with results that are just as good
(if not better) than those same algorithms.

\subsection{Nearest Neighbours}
The Nearest Neighbours, or k-Nearest Neighbours algorithm is a classification
algorithm that assigns a label to an item based on the labels of the existing
points closest to it. Our implementation of Nearest Neighbours proceeds as
follows:
\begin{enumerate}
    \item Create a distance matrix of every point in the entire dataset. 
    Using the reshape2 package, convert it from wide to long format, i.e.
    convert it into a three-column dataframe called data-dist.
    \item Filter this distance table by performing a join on the training set
    by row id. Filter it further by matching the second column in the 
    distance matrix with the row ids of the testing set. Group the distances by the 
    top k distances and the row ids of the testing set so that you have the k nearest
    neighbours of the rows in the testing set. 
    \item Weigh the distances between each training set row-testing set row pair with
    the formula \[weight = 1/distance^2\]
    \item Get the proposed label for each row in the testing set by summing the weights
    for each label and picking the label with the most 'votes'
\end{enumerate}
\subsubsection{Observations}

\begin{table}[H]
\begin{tabular}{|c|c|c|c|c|}
\hline 
Data set & Accuracy & Precision & Recall & F-1 score\tabularnewline
\hline 
\hline 
dataset1 & 0.929 & 0.956 & 0.931 & 0.943\tabularnewline
\hline 
dataset2 & 0.638 &  0.819 & 0.685 &  0.744\tabularnewline
\hline 
\end{tabular}
\caption{Nearest Neighbours performance scores}
\end{table}

The above table shows us the performance metric for the nearest-neighbours algorithm
with $k=10$. We observe fairly high metrics for the first dataset and reduced ones for 
the second dataset. The high performance of nearest neighbours on the first dataset
suggests a uniformity in how the classes spread out among the data points and a 
minimal presence of outliers. In the case of the second dataset, the lower performance
could be because of the presence of a feature with a binary value in opposition to the 
other features' continuous data. The binary value is converted to a numeric one by simply 
mapping them to 1 and 0 but it is still in contrast with the remaining features. 

The nearest neighbour algorithm's biggest strength is its simplicity. It does not rely
on complicated computations, even if the calculation of the distances might be 
computationally expensive. However, it is highly reliant on points close together having
the same label. For a dataset in which labels do not depend on the proximity of points,
nearest neighbours would not be very effective. 

\subsection{Naive Bayesian Classification}
This classification algorithm uses Bayes' theorem of conditional probability to 
estimate the likelihood of a data point belonging to one class versus the other. We
have implemented this algorithm as follows:
\begin{enumerate}
    \item First, we convert the continuous data into a more discrete form. In our
    implementation, we consider 3 quantile cuts for the data and call the ranges
    'Low', 'Medium' and 'High'. In the second dataset, the one row of non-numeric
    data is left as is.
    \item The training set is split into two seperate data frames data-h0 and data-h1, 
    one with all labels 0 and another with all labels 1. We also calculate the class
    prior probabilities, ph0 and ph1
    \item For each column of data in the data-h0 dataframe, we add one row of
    'Low', 'Medium' and 'High' values for Laplacian correction. 
    \item We then calculate the number of occurrences of 'Low', 'Medium' and 'High' and
    divide it by the number of rows in data-h0. This is our conditional probability of 
    the column. These probabilities are then applied to the corresponding column of the
    testing set. 
    \item The probabilities applied to the columns of the testing set are multiplied rowise.
    This is our descriptor posterior probability for the class 0. We multiply each value 
    in the new column by ph0. 
    \item The preceding three steps are repeated for the class 1.
    \item For each row, the (descriptor posterior probability * prior probability) values are
    compared. If the value for class 0 is greater, then that row is assigned to class 0,
    and if not, then it belongs to class 1. 
\end{enumerate}
\subsubsection{Observations}

\begin{table}[H]
\begin{tabular}{|c|c|c|c|c|}
\hline 
Data set & Accuracy & Precision & Recall & F-1 score\tabularnewline
\hline 
\hline 
dataset1 & 0.940 & 0.966 & 0.937 & 0.951\tabularnewline
\hline 
dataset2 & 0.692 &  0.744 & 0.771 &  0.755\tabularnewline
\hline 
\end{tabular}
\caption{Naive Bayesian performance scores}
\end{table}

The above are the performance metrics for Naive Bayesian classification. It shows very good
results on the first dataset, and a reduced performance on the second one. The implication 
here is in the first data set, higher descriptor posterior probabilities correlate very strongly with
the associated class, while this relationship is weaker in the second dataset. This could also be 
attributed to the second dataset having fewer features to work with, giving the classifier less
information to work with.

The advantages of Naive Bayes classifiers is that they are simple to work with and are on solid
mathematical footing, i.e. they are well-grounded in theory. They show good performance 
on most datasets, including ones with many features and are not computationally expensive.

However, the biggest drawback of this classifier is the independence hypothesis i.e.
assuming that the features have no relation to each other. If this is not the case and the
features are correlated, then the classifier may not yield the best results. Additionally,
when converting the continuous data into a discrete form, the method of conversion would also
affect the results. Though we have received fairly good results for both datasets, if we 
were to change our method of conversion, the results would presumably change.

\section{Competiton}
For the Kaggle competition, we used a combination of different methods, followed by 
bootstrap aggregation (bagging) to select the best results for each rows. The 
following methods were used (accuracy in brackets):
\begin{enumerate}
    \item SVM with a linear classification kernel. (0.832)
    \item Random forests with PCA to consider the first 50 features, 100 trees, 
    entropy, max depth 8, min samples split 10 (0.844)
    \item Random forests with PCA - 70 features, 100 trees, gini, max depth 8, min samples split 10 (0.848)
    \item Random forests with PCA - 70 features, 100 trees, gini, max depth 8, min samples split 5 (0.841)
    \item Random forests with PCA - 70 features, 100 trees, gini, max depth 10, min samples split 5 (0.847)
\end{enumerate}
The results of these models were aggregated and the majority label for each row was considered 
for the final result. This has given us a score of 0.852. In our investigations, we found that neither
nearest neighbours, nor Naive Bayesian were yielding very strong results. In regards to SVM, both the
linear and radial kernels were showing similar performance, so for the sake of simplicity, we selected
the linear kernel. The Random Forests algorithm has shown consistently strong results, and had 
many parameters to tune which is why we have made heavy use of it. 

\section{Conclusion}
For this project, we have implemented four different classification algorithms, Decision Trees,
Random Forests, Nearest Neighbours and Naive Bayes Classifiers and evaluated their performance
with accuracy, precision, recall and F1 score using 10-fold cross-validation on two different
datasets. Interpreting the results of each algorithm illustrated the strengths and weaknesses
of each algorithm. We were able to use this knowledge along with the technique of bagging to 
obtain some improvements on our scores in the Kaggle competition for this project.

We also see that while every algorithm had similar scores for the first dataset, the same does not hold true
for the second dataset. This could be explained by the fact that the first dataset has a lot of features, and
in general seems to be easier to split compared to the second, while the second dataset has comparatively fewer features,
and seems to be more cohesive. The second dataset performs especially poorly with the decision tree and
random forest approaches, corroborating our assumption that it might be very cohesive, and difficult to split.
This could be why the kNN and Naive Bayesian approaches have higher scores than the decision tree and random forest
approaches, as they do not rely on learning a decision boundary for classification.
\end{document}
